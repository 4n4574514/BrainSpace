{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nTutorial 3: Null models for gradient significance\n==================================================\nIn this tutorial we assess the significance of correlations between the first\ncanonical gradient and data from other modalities (curvature, cortical\nthickness and T1w/T2w image intensity). A normal test of the significance of\nthe correlation cannot be used, because the spatial auto-correlation in MRI\ndata may bias the test statistic. In this tutorial we will show two approaches\nfor null hypothesis testing: spin permutations and Moran spectral\nrandomization.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>When using either approach to compare gradients to non-gradient markers,\n    we recommend randomizing the non-gradient markers as these randomizations\n    need not maintain the statistical independence between gradients.</p></div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we use the spin permutations approach previously proposed in\n`(Alexander-Bloch et al., 2018)\n<https://www.sciencedirect.com/science/article/pii/S1053811918304968>`_,\nwhich preserves the auto-correlation of the permuted feature(s) by rotating\nthe feature data on the spherical domain.\nWe will start by loading the conte69 surfaces for left and right hemispheres,\ntheir corresponding spheres, midline mask, and t1w/t2w intensity as well as\ncortical thickness data, and a template functional gradient.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\nwarnings.simplefilter('ignore')\n\nimport numpy as np\nfrom brainspace.datasets import load_gradient, load_marker, load_conte69\n\n# load the conte69 hemisphere surfaces and spheres\nsurf_lh, surf_rh = load_conte69()\nsphere_lh, sphere_rh = load_conte69(as_sphere=True)\n\n# Load the data\nt1wt2w_lh, t1wt2w_rh = load_marker('t1wt2w')\nt1wt2w = np.concatenate([t1wt2w_lh, t1wt2w_rh])\n\nthickness_lh, thickness_rh = load_marker('thickness')\nthickness = np.concatenate([thickness_lh, thickness_rh])\n\n# Template functional gradient\nembedding = load_gradient('fc', idx=0, join=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u2019s first generate some null data using spintest.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom brainspace.null_models import SpinPermutations\nfrom brainspace.plotting import plot_hemispheres\n\n# Let's create some rotations\nn_permutations = 10\n\nsp = SpinPermutations(n_rep=n_permutations, random_state=0)\nsp.fit(sphere_lh, points_rh=sphere_rh)\n\nt1wt2w_rotated = np.hstack(sp.randomize(t1wt2w_lh, t1wt2w_rh))\nthickness_rotated = np.hstack(sp.randomize(thickness_lh, thickness_rh))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As an illustration of the rotation, let\u2019s plot the original t1w/t2w data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Plot original data\nplot_hemispheres(surf_lh, surf_rh, array_name=t1wt2w, size=(800, 150), cmap='viridis',\n                 nan_color=(0.5, 0.5, 0.5, 1), color_bar=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "as well as a few rotated versions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Plot some rotations\nplot_hemispheres(surf_lh, surf_rh, array_name=t1wt2w_rotated[:3], size=(800, 450),\n                 cmap='viridis', nan_color=(0.5, 0.5, 0.5, 1), color_bar=True,\n                 label_text=['Rot0', 'Rot1', 'Rot2'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we simply compute the correlations between the first gradient and the\noriginal data, as well as all rotated data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy.stats import pearsonr, spearmanr\n\nfeats = {'t1wt2w': t1wt2w, 'thickness': thickness}\nrotated = {'t1wt2w': t1wt2w_rotated, 'thickness': thickness_rotated}\n\nfor fn, rot in rotated.items():\n    r_spin = np.empty(n_permutations)\n    for i, r in enumerate(t1wt2w_rotated):\n        r_spin[i] = spearmanr(embedding, rot[i], nan_policy='omit')[0]\n\n    r_obs, pv_obs = spearmanr(feats[fn], embedding, nan_policy='omit')\n    pv_spin = (np.count_nonzero(r_spin > r_obs) + 1) / (n_permutations + 1)\n\n    print('{0}:\\n Orig: {1:.5e}\\n Spin: {2:.5e}'.format(fn.capitalize(),\n                                                        pv_obs, pv_spin))\n    print()\n\n# mask = ~np.isnan(thickness)\n# r_spin = {k: np.empty(n_permutations) for k in feats.keys()}\n# for fn, feat in feats.items():\n#\n#     r_spin = np.empty(n_permutations)\n#     for i in range(n_permutations):\n#         # Remove non-cortex\n#         mask_rot = mask & ~np.isnan(rotated[fn][i])\n#         emb = embedding[mask_rot]\n#         r_spin[i] = pearsonr(rotated[fn][i][mask_rot], emb)[0]\n#\n#     r_obs, pv_obs = pearsonr(feat[mask], embedding[mask])\n#     pv_spin = (np.count_nonzero(r_spin > r_obs) + 1) / (n_permutations + 1)\n#\n#     print('{0}:\\n Orig: {1:.5e}\\n Spin: {2:.5e}'.format(fn.capitalize(),\n#                                                         pv_obs, pv_spin))\n#     print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is interesting to see that both p-values increase when taking into\nconsideration the auto-correlation present in the surfaces. Also, we can see\nthat the correlation with thickness is no longer statistically significant\nafter spin permutations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# ###############################################################################\n# # Moran Spectral Randomization (MSR) computes Moran's I, a metric for spatial\n# # auto-correlation and generates normally distributed data with similar\n# # auto-correlation. MSR relies on a weight matrix denoting the spatial\n# # proximity of features to one another. Within neuroimaging, one\n# # straightforward example of this is inverse geodesic distance i.e. distance\n# # along the cortical surface.\n# #\n# # In this example we will show how to use MSR to assess statistical\n# # significance between cortical markers (here curvature and cortical t1wt2w\n# # intensity) and the first functional connectivity gradient. We will start by\n# # loading the left temporal lobe mask, t1w/t2w intensity as well as cortical\n# # thickness data, and a template functional gradient\n#\n#\n# from brainspace.datasets import load_curvature, load_mask\n# from brainspace.mesh import mesh_elements as me\n#\n# mask_tl = load_mask(region='temporal')[:n_pts_lh]\n#\n# # Keep only the temporal lobe.\n# embedding_tl = embedding[:n_pts_lh][mask_tl]\n# t1wt2w_tl = t1wt2w[:n_pts_lh][mask_tl]\n# curv_tl = load_curvature()[:n_pts_lh][mask_tl]\n#\n#\n# ###############################################################################\n# # We will now compute the Moran eigenvectors. This can be done either by\n# # providing a weight matrix of spatial proximity between each vertex, or by\n# # providing a cortical surface. Here we\u2019ll use a cortical surface.\n#\n# from brainspace.null_models import MoranSpectralRandomization\n#\n# # compute spatial weight matrix\n# w = me.get_ring_distance(surf_lh, n_ring=1)\n# w = w[mask_tl][:, mask_tl]\n# w.data **= -1\n#\n# n_rand = 1000\n#\n# msr = MoranSpectralRandomization(n_rep=n_rand, tol=1e-6, random_state=43)\n# msr.fit(w)\n#\n#\n# ###############################################################################\n# # Using the Moran eigenvectors we can now compute the randomized data.\n#\n# curv_rand = msr.randomize(curv_tl)\n# t1wt2w_rand = msr.randomize(t1wt2w_tl)\n#\n#\n# ###############################################################################\n# # Now that we have the randomized data, we can compute correlations between\n# # the gradient and the real/randomised data.\n#\n# from scipy.stats import pearsonr\n# from scipy.spatial.distance import cdist\n#\n# r_orig_curv = pearsonr(curv_tl, embedding_tl)[0]\n# r_rand_curv = 1 - cdist(curv_rand, embedding_tl[None], metric='correlation')\n#\n# r_orig_t1wt2w = pearsonr(t1wt2w_tl, embedding_tl)[0]\n# r_rand_t1wt2w = 1 - cdist(t1wt2w_rand, embedding_tl[None], metric='correlation')\n#\n#\n# ###############################################################################\n# # Finally, the p-values can be computed using the same approach used with\n# # spin permutations."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}