{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nNull models based on spin permutations\n=================================================\n\nIn this example we assess the significance of correlations between the first\ncanonical gradient and data from other modalities (cortical thickness and\nT1w/T2w image intensity). We use the spin test approach previously proposed in\n(Alexander-Bloch et al., 2018), which preserve the auto-correlation of the\npermuted feature(s) by rotating the feature data on the spherical domain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will start by loading the conte69 surfaces for left and right hemispheres\nand their corresponding spheres.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from brainspace.data.base import load_conte69, load_mask\n\n\n# load the conte69 hemisphere surfaces and spheres\nsurf_lh, surf_rh = load_conte69()\nsphere_lh, sphere_rh = load_conte69(as_sphere=True)\n\n# and the mask\nmask = load_mask()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to work with Schaefer parcellations to reduced the computational\nburden.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from brainspace.gradient import GradientMaps\nfrom brainspace.data.base import (load_t1t2, load_thickness, load_parcellation,\n                                  load_group_hcp)\n\nparcel_name = 'schaefer'\nn_parcels = 400\nparcellation = load_parcellation(parcel_name, n_parcels=n_parcels)\n\n\n# load  myelin and thickness data\nmyelin = load_t1t2(parcellation, mask=mask)\nthickness = load_thickness(parcellation, mask=mask)\n\n# and connectivity matrix to compute first canonical gradient\ncm = load_group_hcp(parcel_name, n_parcels=n_parcels)\n\ngm = GradientMaps(n_gradients=1, approach='dm', kernel='normalized_angle',\n                  random_state=16)\ngradient = gm.fit(cm).gradients_[:, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see the the data. We are going to append the data to both hemispheres\nsurfaces and spheres.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom brainspace.plotting import plot_hemispheres\nfrom brainspace.utils.parcellation import map_to_labels\n\n# Append data to surfaces and spheres\nmap_feat = dict(zip(['myelin', 'thickness', 'gradient'],\n                    [myelin, thickness, gradient]))\n\nn_pts_lh = surf_lh.n_points\nfor fn, feat_parc in map_feat.items():\n    feat = map_to_labels(feat_parc, parcellation, mask=mask, fill=np.nan)\n\n    surf_lh.append_array(feat[:n_pts_lh], name=fn, at='p')\n    surf_rh.append_array(feat[n_pts_lh:], name=fn, at='p')\n\n    sphere_lh.append_array(feat[:n_pts_lh], name=fn, at='p')\n    sphere_rh.append_array(feat[n_pts_lh:], name=fn, at='p')\n\nplot_hemispheres(surf_lh, surf_rh,\n                 array_name=['myelin', 'thickness', 'gradient'],\n                 interactive=False, embed_nb=True, size=(800, 600),\n                 cmap_name=['YlOrBr_r', 'PuOr_r', 'viridis'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also see the data on the spheres.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_hemispheres(sphere_lh, sphere_rh,\n                 array_name=['myelin', 'thickness', 'gradient'],\n                 interactive=False, embed_nb=True, size=(800, 600),\n                 cmap_name=['YlOrBr_r', 'PuOr_r', 'viridis'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because we are using a parcellation, we need to compute the centroids for each\nparcels and used them as the sphere coordinates\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from brainspace.mesh import array_operations as aop\n\nmask_lh = mask[:n_pts_lh]\nmask_rh = mask[n_pts_lh:]\n\nparcellation_lh = parcellation[:n_pts_lh]\nparcellation_rh = parcellation[n_pts_lh:]\n\n\n# Compute parcellation centroids and append to spheres\naop.get_parcellation_centroids(sphere_lh, parcellation_lh, mask=mask_lh,\n                               non_centroid=0, append=True,\n                               array_name='centroids')\naop.get_parcellation_centroids(sphere_rh, parcellation_rh, mask=mask_rh,\n                               non_centroid=0, append=True,\n                               array_name='centroids')\n\nmask_centroids_lh = sphere_lh.get_array('centroids') > 0\nmask_centroids_rh = sphere_rh.get_array('centroids') > 0\n\ncentroids_lh = sphere_lh.Points[mask_centroids_lh]\ncentroids_rh = sphere_lh.Points[mask_centroids_rh]\n\n# We can see the centroids on the sphere surfaces\nplot_hemispheres(sphere_lh, sphere_rh, array_name='centroids',\n                 interactive=False, embed_nb=True, size=(800, 200),\n                 cmap_name='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's generate 2000 random samples using spin permutations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from brainspace.null_models import SpinRandomization\n\nn_spins = 2000\nsp = SpinRandomization(n_rep=n_spins, random_state=0)\nsp.fit(centroids_lh, points_rh=centroids_rh)\ngradient_spins_lh, gradient_spins_rh = sp.randomize(gradient[:200],\n                                                    x_rh=gradient[200:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check the 3 first spin permutations\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# First, append randomized data to spheres\nfor i in range(3):\n    array_name = 'gradient_spins{i}'.format(i=i)\n    gs2 = map_to_labels(gradient_spins_lh[i], parcellation_lh, mask=mask_lh,\n                        fill=np.nan)\n    sphere_lh.append_array(gs2, name=array_name, at='p')\n\n    gs2 = map_to_labels(gradient_spins_rh[i], parcellation_rh, mask=mask_rh,\n                        fill=np.nan)\n    sphere_rh.append_array(gs2, name=array_name, at='p')\n\n\n# and plot original data and the 3 first randomizations\narray_names = ['gradient', 'gradient_spins0', 'gradient_spins1',\n               'gradient_spins2']\nplot_hemispheres(sphere_lh, sphere_rh, array_name=array_names,\n                 interactive=False, embed_nb=True, size=(800, 800),\n                 cmap_name='viridis_r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we assess the correlation significance between myelin/thickness and\nthe first canonical gradient without considering the spatial auto-correlation\nin and after accounting for this using spin permutations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy.stats import pearsonr\nfrom scipy.spatial.distance import cdist\n\nfeats = {'myelin': myelin, 'thickness': thickness}\n\nfor fn, feat in feats.items():\n    corr, pv = pearsonr(gradient, feat)\n\n    gradient_spins = np.hstack([gradient_spins_lh, gradient_spins_rh])\n    corr_spin = 1 - cdist(gradient_spins, feat[None],\n                          metric='correlation').squeeze()\n    pv_spin = (np.count_nonzero(corr_spin > corr) + 1) / (corr_spin.size + 1)\n\n    print('{0}:\\n Orig: {1:.5e}\\n Spin: {2:.5e}'.format(fn.capitalize(), pv,\n                                                        pv_spin))\n    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is interesting to see that both p-values increase when taking into\nconsideration the auto-correlation present in the surfaces. Also, we can see\nthat the correlation with thickness is no longer statistically significant\nafter spin permutations.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}